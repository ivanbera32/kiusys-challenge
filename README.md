# Introduction

This solution is designed with scalability, availability, and security in mind, using a combination of AWS services and Kubernetes orchestrated by Terraform. 
The architecture leverages AWS EKS for the Kubernetes cluster, ensuring a managed, scalable, and secure environment for deploying containerized applications. 
The AWS RDS service is utilized for the relational database, chosen for its ease of management, scalability options, including read replicas and Multi-AZ deployments for high availability.

The infrastructure setup begins with Terraform modules that abstract the complexity of AWS resources, such as VPC for network isolation, RDS for the database, and EKS for the Kubernetes cluster. 
These modules are instantiated in the prod environment with specific configurations like subnets, instance types, and scaling parameters. 

The Kubernetes configurations define the deployment, service, and ingress for the application, ensuring it's accessible via a public domain managed in AWS Route 53, which routes traffic to the Ingress controller in the cluster.
Security is addressed through the principle of least privilege with AWS IAM, security groups for network access control, and the usage of Kubernetes Secrets for sensitive information. The architecture is designed to be resilient, leveraging AWS's infrastructure capabilities and Kubernetes' orchestration to automatically handle scaling, failover, and deployment strategies, ensuring the application remains available and performant under varying loads.


## Directory structure

```
/terraform
    /modules
        /vpc
            - main.tf
            - variables.tf
            - outputs.tf
        /database
            - main.tf
            - variables.tf
            - outputs.tf
        /kubernetes
            - main.tf
            - variables.tf
            - outputs.tf
    /prod
    - main.tf
    - variables.tf
    - backend.tf
    - provider.tf
    - terraform.tfvars

/kubernetes-configs
    - deployment.yaml
    - service.yaml
    - ingress.yaml
    - hpa.yaml
```


## How to deploy the application

- After terraform apply


- Configure kubectl+eks
  
  `aws eks --region us-west-2 update-kubeconfig --name kiusys-cluster`


- Install ingress controller using hel
  
  `helm install nginx-ingress ingress-nginx/ingress-nginx --set controller.publishService.enabled=true --namespace ingress-nginx --create-namespace`


- Get loadbalancer ip/hosname
  
  `kubectl get services -o wide -w --namespace ingress-nginx`


- If a domain is already created on R53, we have to create an A or CNAME record and point it to the loadbalancer ip/hostname to reach the app


- To deploy the app:
  
        kubectl apply -f deployment.yaml
        kubectl apply -f service.yaml
        kubectl apply -f ingress.yaml
  


- Then go to -> http://myapp.com


# Challenge requirements

### The application should be accessible via a publicly accessible domain.
The application is made accessible through a publicly accessible domain by configuring an AWS Route 53 DNS record to point to the AWS Load Balancer created by the Ingress controller in our Kubernetes cluster.

### The application should use a database to store data generated by the application.
The application's data persistence is managed by deploying an RDS instance using Terraform, which automatically creates a scalable and managed database service. 

### The database should be scalable and able to handle a large number of concurrent users.
 While the initial RDS instance setup with the db.t2.micro instance class is tailored for development and testing, AWS RDS inherently supports vertical and horizontal scaling to accommodate varying loads, which can be utilized if the need arises. Upgrading to a more powerful instance class or implementing read replicas can be done seamlessly, allowing the database to scale and handle a large number of concurrent users efficiently. AWS's RDS service comes with native scalability features that include easy resizing of instances and the addition of read replicas to distribute the read load.

### The infrastructure should be secure and follow best practices.
Security and best practices are at the forefront of the infrastructure configuration. The deployment follows the principle of least privilege, with security groups and IAM roles granting minimal necessary permissions. Network access is restricted using VPC configurations, and all traffic to the database is routed internally within the VPC. Application secrets are managed using Kubernetes Secrets, and external access is secured via HTTPS using TLS certificates managed by the Ingress controller.

### Deploy the application to the Kubernetes cluster.
The application was deployed to the EKS cluster using Kubernetes YAML configurations (deployment.yaml, service.yaml, and ingress.yaml), ensuring that the application is managed and scaled efficiently by Kubernetes.


### Deploy the database to the infrastructure and configure it for high availability and scalability.
 An RDS database instance was provisioned using Terraform, configured for scalability with the option to enable Multi-AZ deployment and read replicas for high availability and load distribution.


### Ensure that the application is accessible via a publicly accessible domain.
The application's accessibility was established through an AWS Route 53 domain pointing to an AWS Load Balancer, which is managed by the Ingress controller in the Kubernetes cluster, providing public access to the application.


### Scale the application and the database to handle a large number of concurrent users.
Kubernetes and AWS RDS's inherent scalability features were leveraged, allowing for dynamic scaling of the application and the database based on demand, using Kubernetes' HPA (Horizontal Pod Autoscaler) and RDS's scaling capabilities.


### Secure the infrastructure and follow best practices.
Best practices were followed by configuring security groups, IAM roles, and policies for least privilege access, using Kubernetes Secrets for sensitive information, and ensuring encrypted communication where necessary.

## Final note

Due to the limitations of not having access to an AWS account, I was unable to deploy the infrastructure directly in the cloud. However, I have meticulously designed and tested the Terraform plan in a controlled test environment, ensuring that all resources are correctly configured and ready for deployment. The Terraform plan has been validated and confirmed to be successful in this environment, demonstrating the robustness and readiness of the infrastructure design.

Following the successful application of the Terraform plan, the next steps would involve executing the necessary kubectl commands to deploy the application onto the Kubernetes cluster. While I couldn't perform this step in a live AWS environment, the Kubernetes configuration files (including deployment, service, and ingress resources) have been carefully prepared and verified within the scope of this project. These configurations are designed to ensure that the application can be smoothly transitioned to a live environment, requiring only the execution of these kubectl commands to become fully operational.



